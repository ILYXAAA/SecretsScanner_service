#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ —Å–µ–∫—Ä–µ—Ç–æ–≤
–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç—ã —Å–µ–∫—Ä–µ—Ç–æ–≤ –∏ –Ω–µ-—Å–µ–∫—Ä–µ—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ª—É—á—à–∏—Ö –ø—Ä–∞–∫—Ç–∏–∫ ML

–ê–≤—Ç–æ—Ä: AI Assistant
–í–µ—Ä—Å–∏—è: 2.0 - —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞–º–∏ –∏ –∞–≤—Ç–æ–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º
"""

import os
import re
import yaml
import json
import hashlib
import statistics
import shutil
from pathlib import Path
from collections import Counter, defaultdict
from typing import List, Dict, Tuple, Set
import difflib
from datetime import datetime

try:
    from tqdm import tqdm
except ImportError:
    print("–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é tqdm –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞...")
    os.system("pip install tqdm")
    from tqdm import tqdm


class DatasetQualityAnalyzer:
    """
    –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ —Å–µ–∫—Ä–µ—Ç–æ–≤
    """
    
    def __init__(self, rules_path: str, secrets_path: str, non_secrets_path: str):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
        
        Args:
            rules_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å —Ä–µ–≥—É–ª—è—Ä–Ω—ã–º–∏ –≤—ã—Ä–∞–∂–µ–Ω–∏—è–º–∏
            secrets_path: –ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É —Å —Å–µ–∫—Ä–µ—Ç–∞–º–∏
            non_secrets_path: –ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É —Å –Ω–µ-—Å–µ–∫—Ä–µ—Ç–∞–º–∏
        """
        self.rules_path = rules_path
        self.secrets_path = secrets_path
        self.non_secrets_path = non_secrets_path
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        print("–ó–∞–≥—Ä—É–∂–∞—é –ø—Ä–∞–≤–∏–ª–∞...")
        self.rules = self._load_rules()
        
        print("–ó–∞–≥—Ä—É–∂–∞—é –¥–∞—Ç–∞—Å–µ—Ç —Å–µ–∫—Ä–µ—Ç–æ–≤...")
        self.secrets = self._load_dataset(secrets_path)
        
        print("–ó–∞–≥—Ä—É–∂–∞—é –¥–∞—Ç–∞—Å–µ—Ç –Ω–µ-—Å–µ–∫—Ä–µ—Ç–æ–≤...")
        self.non_secrets = self._load_dataset(non_secrets_path)
        
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(self.secrets)} —Å–µ–∫—Ä–µ—Ç–æ–≤, {len(self.non_secrets)} –Ω–µ-—Å–µ–∫—Ä–µ—Ç–æ–≤")
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        self.analysis_results = {}
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
        # (—É–±—Ä–∞–ª–∏ –∞–Ω–∞–ª–∏–∑ –ø–æ—Ö–æ–∂–∏—Ö —Å—Ç—Ä–æ–∫ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏)
        
    def _load_rules(self) -> List[Dict]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–∞–≤–∏–ª–∞ –∏–∑ YAML —Ñ–∞–π–ª–∞"""
        try:
            with open(self.rules_path, 'r', encoding='utf-8') as f:
                rules = yaml.safe_load(f)
            return rules if isinstance(rules, list) else []
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–∞–≤–∏–ª: {e}")
            return []
    
    def _load_dataset(self, path: str) -> List[str]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            with open(path, 'r', encoding='utf-8') as f:
                lines = []
                for line in tqdm(f, desc=f"–ß–∏—Ç–∞—é {os.path.basename(path)}"):
                    line = line.strip()
                    if line:
                        lines.append(line)
                return lines
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ {path}: {e}")
            return []
    
    def analyze_basic_statistics(self) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –±–∞–∑–æ–≤–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
        print("–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –±–∞–∑–æ–≤—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É...")
        
        stats = {
            'secrets_count': len(self.secrets),
            'non_secrets_count': len(self.non_secrets),
            'total_count': len(self.secrets) + len(self.non_secrets),
            'class_balance': len(self.secrets) / (len(self.secrets) + len(self.non_secrets)) if self.secrets or self.non_secrets else 0,
            'secrets_avg_length': statistics.mean([len(s) for s in self.secrets]) if self.secrets else 0,
            'non_secrets_avg_length': statistics.mean([len(s) for s in self.non_secrets]) if self.non_secrets else 0,
            'secrets_length_std': statistics.stdev([len(s) for s in self.secrets]) if len(self.secrets) > 1 else 0,
            'non_secrets_length_std': statistics.stdev([len(s) for s in self.non_secrets]) if len(self.non_secrets) > 1 else 0
        }
        
        self.analysis_results['basic_statistics'] = stats
        return stats
    
    def analyze_duplicates(self) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö"""
        print("–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –¥—É–±–ª–∏–∫–∞—Ç—ã...")
        
        # –¢–æ—á–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
        print("  –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞—é –¥—É–±–ª–∏–∫–∞—Ç—ã –≤ —Å–µ–∫—Ä–µ—Ç–∞—Ö...")
        secrets_counts = Counter(tqdm(self.secrets, desc="–ê–Ω–∞–ª–∏–∑ —Å–µ–∫—Ä–µ—Ç–æ–≤"))
        
        print("  –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞—é –¥—É–±–ª–∏–∫–∞—Ç—ã –≤ –Ω–µ-—Å–µ–∫—Ä–µ—Ç–∞—Ö...")
        non_secrets_counts = Counter(tqdm(self.non_secrets, desc="–ê–Ω–∞–ª–∏–∑ –Ω–µ-—Å–µ–∫—Ä–µ—Ç–æ–≤"))
        
        secrets_duplicates = {k: v for k, v in secrets_counts.items() if v > 1}
        non_secrets_duplicates = {k: v for k, v in non_secrets_counts.items() if v > 1}
        
        # –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏
        print("  –ò—â—É –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏...")
        secrets_set = set(self.secrets)
        non_secrets_set = set(self.non_secrets)
        intersection = secrets_set & non_secrets_set
        
        # –£–±–∏—Ä–∞–µ–º –∞–Ω–∞–ª–∏–∑ –ø–æ—Ö–æ–∂–∏—Ö —Å—Ç—Ä–æ–∫ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        
        duplicates_analysis = {
            'secrets_exact_duplicates': len(secrets_duplicates),
            'non_secrets_exact_duplicates': len(non_secrets_duplicates),
            'cross_class_duplicates': len(intersection),
            'secrets_duplicate_ratio': len(secrets_duplicates) / len(self.secrets) if self.secrets else 0,
            'non_secrets_duplicate_ratio': len(non_secrets_duplicates) / len(self.non_secrets) if self.non_secrets else 0,
            'duplicate_examples': {
                'secrets': list(secrets_duplicates.keys())[:5],
                'non_secrets': list(non_secrets_duplicates.keys())[:5],
                'cross_class': list(intersection)[:5]
            },
            'secrets_duplicates_dict': secrets_duplicates,
            'non_secrets_duplicates_dict': non_secrets_duplicates,
            'cross_class_list': list(intersection)
        }
        
        self.analysis_results['duplicates'] = duplicates_analysis
        return duplicates_analysis
    
    def analyze_rule_coverage(self) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –ø–æ–∫—Ä—ã—Ç–∏—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –ø—Ä–∞–≤–∏–ª–∞–º–∏"""
        print("–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –ø–æ–∫—Ä—ã—Ç–∏–µ –ø—Ä–∞–≤–∏–ª–∞–º–∏...")
        
        rule_matches = defaultdict(lambda: {'secrets': 0, 'non_secrets': 0})
        uncovered_secrets = []
        uncovered_non_secrets = []
        
        # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è
        compiled_rules = []
        for rule in tqdm(self.rules, desc="–ö–æ–º–ø–∏–ª–∏—Ä—É—é –ø—Ä–∞–≤–∏–ª–∞"):
            try:
                compiled_rules.append({
                    'id': rule['id'],
                    'pattern': re.compile(rule['pattern'], re.IGNORECASE),
                    'severity': rule.get('severity', 'UNKNOWN')
                })
            except re.error as e:
                print(f"–û—à–∏–±–∫–∞ –≤ —Ä–µ–≥—É–ª—è—Ä–Ω–æ–º –≤—ã—Ä–∞–∂–µ–Ω–∏–∏ {rule['id']}: {e}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–∫—Ä—ã—Ç–∏–µ —Å–µ–∫—Ä–µ—Ç–æ–≤
        for secret in tqdm(self.secrets, desc="–ü—Ä–æ–≤–µ—Ä—è—é –ø–æ–∫—Ä—ã—Ç–∏–µ —Å–µ–∫—Ä–µ—Ç–æ–≤"):
            matched = False
            for rule in compiled_rules:
                if rule['pattern'].search(secret):
                    rule_matches[rule['id']]['secrets'] += 1
                    matched = True
                    break  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –æ–¥–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
            if not matched:
                uncovered_secrets.append(secret)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–∫—Ä—ã—Ç–∏–µ –Ω–µ-—Å–µ–∫—Ä–µ—Ç–æ–≤
        for non_secret in tqdm(self.non_secrets, desc="–ü—Ä–æ–≤–µ—Ä—è—é –ø–æ–∫—Ä—ã—Ç–∏–µ –Ω–µ-—Å–µ–∫—Ä–µ—Ç–æ–≤"):
            matched = False
            for rule in compiled_rules:
                if rule['pattern'].search(non_secret):
                    rule_matches[rule['id']]['non_secrets'] += 1
                    matched = True
                    break  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –æ–¥–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
            if not matched:
                uncovered_non_secrets.append(non_secret)
        
        coverage_analysis = {
            'total_rules': len(compiled_rules),
            'rules_matching_secrets': len([r for r in rule_matches.values() if r['secrets'] > 0]),
            'rules_matching_non_secrets': len([r for r in rule_matches.values() if r['non_secrets'] > 0]),
            'secrets_coverage_ratio': (len(self.secrets) - len(uncovered_secrets)) / len(self.secrets) if self.secrets else 0,
            'non_secrets_coverage_ratio': (len(self.non_secrets) - len(uncovered_non_secrets)) / len(self.non_secrets) if self.non_secrets else 0,
            'uncovered_secrets_count': len(uncovered_secrets),
            'uncovered_non_secrets_count': len(uncovered_non_secrets),
            'rule_matches': dict(rule_matches),
            'uncovered_examples': {
                'secrets': uncovered_secrets[:10],
                'non_secrets': uncovered_non_secrets[:10]
            },
            'uncovered_secrets_list': uncovered_secrets,
            'uncovered_non_secrets_list': uncovered_non_secrets
        }
        
        self.analysis_results['rule_coverage'] = coverage_analysis
        return coverage_analysis
    
    def analyze_data_quality_issues(self) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–±–ª–µ–º –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"""
        print("–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –ø—Ä–æ–±–ª–µ–º—ã –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        issues = {
            'empty_strings': [],
            'too_short': [],
            'too_long': [],
            'non_printable_chars': [],
            'encoding_issues': [],
            'whitespace_only': [],
            'potential_test_data': []
        }
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
        all_data = [('secret', s) for s in self.secrets] + [('non_secret', s) for s in self.non_secrets]
        
        for data_type, string in tqdm(all_data, desc="–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"):
            # –ü—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
            if not string:
                issues['empty_strings'].append((data_type, string))
                continue
            
            # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ (–≤–æ–∑–º–æ–∂–Ω–æ –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ)
            if len(string) < 8:  # –£–≤–µ–ª–∏—á–∏–ª –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É –¥–ª—è —Å–µ–∫—Ä–µ—Ç–æ–≤
                issues['too_short'].append((data_type, string))
            
            # –°–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ (–≤–æ–∑–º–æ–∂–Ω–æ, —Å–æ–¥–µ—Ä–∂–∞—Ç –ª–∏—à–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ)
            if len(string) > 512:  # –†–∞–∑—É–º–Ω—ã–π –ª–∏–º–∏—Ç –¥–ª—è —Å–µ–∫—Ä–µ—Ç–æ–≤
                issues['too_long'].append((data_type, string[:100] + '...'))
            
            # –¢–æ–ª—å–∫–æ –ø—Ä–æ–±–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
            if string.isspace():
                issues['whitespace_only'].append((data_type, repr(string)))
            
            # –ù–µ–ø–µ—á–∞—Ç–∞–µ–º—ã–µ —Å–∏–º–≤–æ–ª—ã
            if any(ord(c) < 32 and c not in '\t\n\r' for c in string):
                issues['non_printable_chars'].append((data_type, repr(string)))
            
            # –í–æ–∑–º–æ–∂–Ω—ã–µ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
            test_indicators = ['test', 'example', 'sample', 'demo', 'fake', 'dummy', 'placeholder', 'lorem', 'ipsum']
            if any(indicator in string.lower() for indicator in test_indicators):
                issues['potential_test_data'].append((data_type, string))
        
        quality_issues = {
            'issues_summary': {k: len(v) for k, v in issues.items()},
            'total_issues': sum(len(v) for v in issues.values()),
            'issues_examples': {k: v[:5] for k, v in issues.items()},
            'issues_full': issues
        }
        
        self.analysis_results['quality_issues'] = quality_issues
        return quality_issues
    
    def analyze_pattern_distribution(self) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö"""
        print("–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤...")
        
        patterns = {
            'contains_digits': 0,
            'contains_uppercase': 0,
            'contains_lowercase': 0,
            'contains_special_chars': 0,
            'contains_spaces': 0,
            'alphanumeric_only': 0,
            'starts_with_prefix': defaultdict(int),
            'common_lengths': defaultdict(int),
            'entropy_distribution': []
        }
        
        # –û–±—â–∏–µ –ø—Ä–µ—Ñ–∏–∫—Å—ã –¥–ª—è API –∫–ª—é—á–µ–π –∏ —Ç–æ–∫–µ–Ω–æ–≤
        common_prefixes = ['sk-', 'pk-', 'api_', 'token_', 'key_', 'secret_', 'auth_', 'bearer_']
        
        all_strings = self.secrets + self.non_secrets
        
        for string in tqdm(all_strings, desc="–ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"):
            if not string:
                continue
                
            # –ë–∞–∑–æ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
            if any(c.isdigit() for c in string):
                patterns['contains_digits'] += 1
            if any(c.isupper() for c in string):
                patterns['contains_uppercase'] += 1
            if any(c.islower() for c in string):
                patterns['contains_lowercase'] += 1
            if any(not c.isalnum() and not c.isspace() for c in string):
                patterns['contains_special_chars'] += 1
            if ' ' in string:
                patterns['contains_spaces'] += 1
            if string.isalnum():
                patterns['alphanumeric_only'] += 1
            
            # –ü—Ä–µ—Ñ–∏–∫—Å—ã
            for prefix in common_prefixes:
                if string.lower().startswith(prefix):
                    patterns['starts_with_prefix'][prefix] += 1
            
            # –î–ª–∏–Ω—ã
            patterns['common_lengths'][len(string)] += 1
            
            # –≠–Ω—Ç—Ä–æ–ø–∏—è (–ø—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞)
            entropy = self._calculate_entropy(string)
            patterns['entropy_distribution'].append(entropy)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —ç–Ω—Ç—Ä–æ–ø–∏–∏
        if patterns['entropy_distribution']:
            patterns['entropy_stats'] = {
                'mean': statistics.mean(patterns['entropy_distribution']),
                'median': statistics.median(patterns['entropy_distribution']),
                'std': statistics.stdev(patterns['entropy_distribution']) if len(patterns['entropy_distribution']) > 1 else 0
            }
        
        # –¢–æ–ø –¥–ª–∏–Ω
        patterns['top_lengths'] = dict(Counter(patterns['common_lengths']).most_common(10))
        patterns['starts_with_prefix'] = dict(patterns['starts_with_prefix'])
        
        self.analysis_results['pattern_distribution'] = patterns
        return patterns
    
    def _calculate_entropy(self, string: str) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —ç–Ω—Ç—Ä–æ–ø–∏—é –®–µ–Ω–Ω–æ–Ω–∞ –¥–ª—è —Å—Ç—Ä–æ–∫–∏"""
        if not string:
            return 0.0
        
        char_counts = Counter(string)
        string_length = len(string)
        entropy = 0.0
        
        for count in char_counts.values():
            probability = count / string_length
            if probability > 0:
                import math
                entropy -= probability * math.log2(probability)
        
        return entropy
    
    def generate_recommendations(self) -> Dict:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
        print("–ì–µ–Ω–µ—Ä–∏—Ä—É—é —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏...")
        
        recommendations = {
            'critical_issues': [],
            'warnings': [],
            'suggestions': [],
            'overall_quality_score': 0.0,
            'quality_score_10': 0.0,  # –û—Ü–µ–Ω–∫–∞ –ø–æ 10-–±–∞–ª—å–Ω–æ–π —à–∫–∞–ª–µ
            'fixable_issues': {}  # –ü—Ä–æ–±–ª–µ–º—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –∏—Å–ø—Ä–∞–≤–∏—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
        }
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        stats = self.analysis_results.get('basic_statistics', {})
        duplicates = self.analysis_results.get('duplicates', {})
        coverage = self.analysis_results.get('rule_coverage', {})
        quality = self.analysis_results.get('quality_issues', {})
        
        # –°–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ (–∫–∞–∂–¥—ã–π –∫—Ä–∏—Ç–µ—Ä–∏–π –∏–∑ 10 –±–∞–ª–ª–æ–≤)
        scores = {
            'dataset_size': 0,      # –†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞
            'class_balance': 0,     # –ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤
            'duplicates': 0,        # –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            'cross_class': 0,       # –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π
            'rule_coverage': 0,     # –ü–æ–∫—Ä—ã—Ç–∏–µ –ø—Ä–∞–≤–∏–ª–∞–º–∏
            'data_quality': 0       # –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö
        }
        
        # 1. –û—Ü–µ–Ω–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ (0-10)
        total_count = stats.get('total_count', 0)
        if total_count >= 50000:
            scores['dataset_size'] = 10
        elif total_count >= 20000:
            scores['dataset_size'] = 8
        elif total_count >= 10000:
            scores['dataset_size'] = 6
        elif total_count >= 5000:
            scores['dataset_size'] = 4
        elif total_count >= 1000:
            scores['dataset_size'] = 2
        else:
            scores['dataset_size'] = 0
        
        # 2. –û—Ü–µ–Ω–∫–∞ –±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤ (0-10)
        balance = stats.get('class_balance', 0.5)
        if 0.4 <= balance <= 0.6:  # –ò–¥–µ–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å
            scores['class_balance'] = 10
        elif 0.3 <= balance <= 0.7:  # –•–æ—Ä–æ—à–∏–π –±–∞–ª–∞–Ω—Å
            scores['class_balance'] = 8
        elif 0.2 <= balance <= 0.8:  # –ü—Ä–∏–µ–º–ª–µ–º—ã–π –±–∞–ª–∞–Ω—Å
            scores['class_balance'] = 6
        elif 0.1 <= balance <= 0.9:  # –ü–ª–æ—Ö–æ–π –±–∞–ª–∞–Ω—Å
            scores['class_balance'] = 3
        else:  # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –ø–ª–æ—Ö–æ–π –±–∞–ª–∞–Ω—Å
            scores['class_balance'] = 0
        
        # 3. –û—Ü–µ–Ω–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (0-10)
        max_dup_ratio = max(duplicates.get('secrets_duplicate_ratio', 0), 
                           duplicates.get('non_secrets_duplicate_ratio', 0))
        if max_dup_ratio <= 0.01:  # ‚â§1% –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            scores['duplicates'] = 10
        elif max_dup_ratio <= 0.05:  # ‚â§5% –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            scores['duplicates'] = 7
        elif max_dup_ratio <= 0.1:   # ‚â§10% –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            scores['duplicates'] = 4
        elif max_dup_ratio <= 0.2:   # ‚â§20% –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            scores['duplicates'] = 2
        else:  # >20% –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            scores['duplicates'] = 0
        
        # 4. –û—Ü–µ–Ω–∫–∞ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏ (0-10)
        cross_class_count = duplicates.get('cross_class_duplicates', 0)
        if cross_class_count == 0:
            scores['cross_class'] = 10
        elif cross_class_count <= total_count * 0.001:  # ‚â§0.1% –æ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞
            scores['cross_class'] = 5
        else:  # >0.1% –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π - –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞
            scores['cross_class'] = 0
        
        # 5. –û—Ü–µ–Ω–∫–∞ –ø–æ–∫—Ä—ã—Ç–∏—è –ø—Ä–∞–≤–∏–ª–∞–º–∏ (0-10)
        secrets_coverage = coverage.get('secrets_coverage_ratio', 0)
        if secrets_coverage >= 0.95:  # ‚â•95% –ø–æ–∫—Ä—ã—Ç–∏–µ
            scores['rule_coverage'] = 10
        elif secrets_coverage >= 0.9:   # ‚â•90% –ø–æ–∫—Ä—ã—Ç–∏–µ
            scores['rule_coverage'] = 8
        elif secrets_coverage >= 0.8:   # ‚â•80% –ø–æ–∫—Ä—ã—Ç–∏–µ
            scores['rule_coverage'] = 6
        elif secrets_coverage >= 0.7:   # ‚â•70% –ø–æ–∫—Ä—ã—Ç–∏–µ
            scores['rule_coverage'] = 4
        elif secrets_coverage >= 0.5:   # ‚â•50% –ø–æ–∫—Ä—ã—Ç–∏–µ
            scores['rule_coverage'] = 2
        else:  # <50% –ø–æ–∫—Ä—ã—Ç–∏–µ
            scores['rule_coverage'] = 0
        
        # 6. –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö (0-10)
        issues_ratio = quality.get('total_issues', 0) / max(total_count, 1)
        if issues_ratio <= 0.01:  # ‚â§1% –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
            scores['data_quality'] = 10
        elif issues_ratio <= 0.05:  # ‚â§5% –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
            scores['data_quality'] = 7
        elif issues_ratio <= 0.1:   # ‚â§10% –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
            scores['data_quality'] = 4
        elif issues_ratio <= 0.2:   # ‚â§20% –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
            scores['data_quality'] = 2
        else:  # >20% –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
            scores['data_quality'] = 0
        
        # –ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ –ø–æ 10-–±–∞–ª—å–Ω–æ–π —à–∫–∞–ª–µ (—Å—Ä–µ–¥–Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω–∞—è)
        weights = {
            'dataset_size': 0.15,
            'class_balance': 0.15,
            'duplicates': 0.15,
            'cross_class': 0.25,  # –°–∞–º—ã–π –≤–∞–∂–Ω—ã–π –∫—Ä–∏—Ç–µ—Ä–∏–π
            'rule_coverage': 0.20,
            'data_quality': 0.10
        }
        
        weighted_score = sum(scores[criterion] * weights[criterion] for criterion in scores)
        recommendations['quality_score_10'] = round(weighted_score, 1)
        
        # –°—Ç–∞—Ä–∞—è 100-–±–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        score = weighted_score * 10  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ 100-–±–∞–ª—å–Ω—É—é
        
        # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
        if total_count < 1000:
            recommendations['critical_issues'].append(
                f"–î–∞—Ç–∞—Å–µ—Ç —Å–ª–∏—à–∫–æ–º –º–∞–ª ({total_count:,} –æ–±—Ä–∞–∑—Ü–æ–≤). –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –º–∏–Ω–∏–º—É–º 10,000+ –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞."
            )
            score -= 20
        
        if cross_class_count > 0:
            recommendations['critical_issues'].append(
                f"–ù–∞–π–¥–µ–Ω–æ {cross_class_count:,} –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫ –≤ –æ–±–æ–∏—Ö –∫–ª–∞—Å—Å–∞—Ö. "
                "–≠—Ç–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ —Ä–∞–∑–º–µ—Ç–∫–∏!"
            )
            recommendations['fixable_issues']['cross_class_duplicates'] = True
            score -= 25
        
        if secrets_coverage < 0.7:
            recommendations['critical_issues'].append(
                f"–ù–∏–∑–∫–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ —Å–µ–∫—Ä–µ—Ç–æ–≤ –ø—Ä–∞–≤–∏–ª–∞–º–∏ ({secrets_coverage:.1%}). "
                "–ú–Ω–æ–≥–∏–µ —Å–µ–∫—Ä–µ—Ç—ã –Ω–µ –¥–µ—Ç–µ–∫—Ç–∏—Ä—É—é—Ç—Å—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –ø—Ä–∞–≤–∏–ª–∞–º–∏."
            )
            score -= 20
        
        # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
        if balance < 0.3 or balance > 0.7:
            recommendations['warnings'].append(
                f"–ù–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã ({balance:.1%} —Å–µ–∫—Ä–µ—Ç–æ–≤). "
                "–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ 30-70% –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è."
            )
            score -= 10
        
        if max_dup_ratio > 0.05:
            recommendations['warnings'].append(
                f"–í—ã—Å–æ–∫–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ ({max_dup_ratio:.1%}). "
                "–î—É–±–ª–∏–∫–∞—Ç—ã –º–æ–≥—É—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—é."
            )
            recommendations['fixable_issues']['duplicates'] = True
            score -= 10
        
        if issues_ratio > 0.05:
            recommendations['warnings'].append(
                f"–ú–Ω–æ–≥–æ –ø—Ä–æ–±–ª–µ–º –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö ({quality.get('total_issues', 0):,} –ø—Ä–æ–±–ª–µ–º). "
                "–¢—Ä–µ–±—É–µ—Ç—Å—è –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö."
            )
            recommendations['fixable_issues']['quality_issues'] = True
            score -= 15
        
        # –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ —É–ª—É—á—à–µ–Ω–∏—é
        if total_count < 10000:
            recommendations['suggestions'].append(
                "–£–≤–µ–ª–∏—á—å—Ç–µ —Ä–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –ª—É—á—à–µ–π –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏."
            )
        
        if coverage.get('uncovered_secrets_count', 0) > 0:
            recommendations['suggestions'].append(
                f"–î–æ–±–∞–≤—å—Ç–µ –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è {coverage.get('uncovered_secrets_count', 0):,} "
                "–Ω–µ–æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö —Å–µ–∫—Ä–µ—Ç–æ–≤ –∏–ª–∏ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –∏—Ö —Ä–∞–∑–º–µ—Ç–∫–∏."
            )
        
        recommendations['overall_quality_score'] = max(0, score)
        recommendations['detailed_scores'] = scores
        self.analysis_results['recommendations'] = recommendations
        
        return recommendations
    
    def create_backups(self):
        """–°–æ–∑–¥–∞–µ—Ç —Ä–µ–∑–µ—Ä–≤–Ω—ã–µ –∫–æ–ø–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        backup_dir = f"backup_{timestamp}"
        os.makedirs(backup_dir, exist_ok=True)
        
        secrets_backup = os.path.join(backup_dir, "Dataset_Secrets_backup.txt")
        non_secrets_backup = os.path.join(backup_dir, "Dataset_NonSecrets_backup.txt")
        
        shutil.copy2(self.secrets_path, secrets_backup)
        shutil.copy2(self.non_secrets_path, non_secrets_backup)
        
        print(f"–°–æ–∑–¥–∞–Ω—ã —Ä–µ–∑–µ—Ä–≤–Ω—ã–µ –∫–æ–ø–∏–∏ –≤ –ø–∞–ø–∫–µ: {backup_dir}")
        return backup_dir
    
    def fix_datasets(self):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø—Ä–∞–≤–ª—è–µ—Ç –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –≤ –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö"""
        print("\n" + "="*60)
        print("–ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –î–ê–¢–ê–°–ï–¢–û–í")
        print("="*60)
        
        if not self.analysis_results:
            print("–°–Ω–∞—á–∞–ª–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–æ–≤–µ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑!")
            return
        
        # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—ã–µ –∫–æ–ø–∏–∏
        backup_dir = self.create_backups()
        
        fixed_secrets = self.secrets.copy()
        fixed_non_secrets = self.non_secrets.copy()
        
        fixes_applied = []
        
        # 1. –£–¥–∞–ª—è–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏ (–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞)
        duplicates = self.analysis_results.get('duplicates', {})
        cross_class = duplicates.get('cross_class_list', [])
        if cross_class:
            print(f"–£–¥–∞–ª—è—é {len(cross_class)} –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏...")
            # –£–¥–∞–ª—è–µ–º –∏–∑ –Ω–µ-—Å–µ–∫—Ä–µ—Ç–æ–≤, —Ç–∞–∫ –∫–∞–∫ —Å–µ–∫—Ä–µ—Ç—ã –≤–∞–∂–Ω–µ–µ
            fixed_non_secrets = [s for s in fixed_non_secrets if s not in cross_class]
            fixes_applied.append(f"–£–¥–∞–ª–µ–Ω—ã {len(cross_class)} –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏")
        
        # 2. –£–¥–∞–ª—è–µ–º —Ç–æ—á–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã
        secrets_dups = duplicates.get('secrets_duplicates_dict', {})
        non_secrets_dups = duplicates.get('non_secrets_duplicates_dict', {})
        
        if secrets_dups:
            print("–£–¥–∞–ª—è—é –¥—É–±–ª–∏–∫–∞—Ç—ã –∏–∑ —Å–µ–∫—Ä–µ—Ç–æ–≤...")
            fixed_secrets = list(set(fixed_secrets))  # –ü—Ä–æ—Å—Ç–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            fixes_applied.append(f"–£–¥–∞–ª–µ–Ω—ã –¥—É–±–ª–∏–∫–∞—Ç—ã —Å–µ–∫—Ä–µ—Ç–æ–≤")
        
        if non_secrets_dups:
            print("–£–¥–∞–ª—è—é –¥—É–±–ª–∏–∫–∞—Ç—ã –∏–∑ –Ω–µ-—Å–µ–∫—Ä–µ—Ç–æ–≤...")
            fixed_non_secrets = list(set(fixed_non_secrets))
            fixes_applied.append(f"–£–¥–∞–ª–µ–Ω—ã –¥—É–±–ª–∏–∫–∞—Ç—ã –Ω–µ-—Å–µ–∫—Ä–µ—Ç–æ–≤")
        
        # 3. –£–¥–∞–ª—è–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
        quality_issues = self.analysis_results.get('quality_issues', {}).get('issues_full', {})
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
        problematic_strings = set()
        
        for issue_type, issues in quality_issues.items():
            if issue_type in ['empty_strings', 'too_short', 'too_long', 'whitespace_only', 
                             'non_printable_chars', 'potential_test_data']:
                for data_type, string in issues:
                    if issue_type == 'too_long':
                        # –î–ª—è —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫ –∏—â–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç—Ä–æ–∫—É
                        original_string = string.replace('...', '')
                        for s in (fixed_secrets + fixed_non_secrets):
                            if s.startswith(original_string):
                                problematic_strings.add(s)
                                break
                    else:
                        problematic_strings.add(string)
        
        if problematic_strings:
            print(f"–£–¥–∞–ª—è—é {len(problematic_strings)} –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Å—Ç—Ä–æ–∫...")
            fixed_secrets = [s for s in fixed_secrets if s not in problematic_strings]
            fixed_non_secrets = [s for s in fixed_non_secrets if s not in problematic_strings]
            fixes_applied.append(f"–£–¥–∞–ª–µ–Ω—ã {len(problematic_strings)} –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Å—Ç—Ä–æ–∫")
        
        # 4. –£–¥–∞–ª—è–µ–º –Ω–µ–æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ —Å–µ–∫—Ä–µ—Ç—ã (–≤–æ–∑–º–æ–∂–Ω–æ, —ç—Ç–æ –ª–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è)
        coverage = self.analysis_results.get('rule_coverage', {})
        uncovered_secrets = coverage.get('uncovered_secrets_list', [])
        
        if uncovered_secrets and len(uncovered_secrets) < len(fixed_secrets) * 0.3:  # –ù–µ –±–æ–ª–µ–µ 30%
            print(f"–£–¥–∞–ª—è—é {len(uncovered_secrets)} –Ω–µ–æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö '—Å–µ–∫—Ä–µ—Ç–æ–≤' (–≤–æ–∑–º–æ–∂–Ω–æ, –ª–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è)...")
            fixed_secrets = [s for s in fixed_secrets if s not in uncovered_secrets]
            fixes_applied.append(f"–£–¥–∞–ª–µ–Ω—ã {len(uncovered_secrets)} –Ω–µ–æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö —Å–µ–∫—Ä–µ—Ç–æ–≤")
        
        # 5. –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        secrets_count = len(fixed_secrets)
        non_secrets_count = len(fixed_non_secrets)
        
        if secrets_count > 0 and non_secrets_count > 0:
            balance = secrets_count / (secrets_count + non_secrets_count)
            if balance < 0.2:  # –°–ª–∏—à–∫–æ–º –º–∞–ª–æ —Å–µ–∫—Ä–µ—Ç–æ–≤
                # –£—Ä–µ–∑–∞–µ–º –Ω–µ-—Å–µ–∫—Ä–µ—Ç—ã
                target_non_secrets = min(non_secrets_count, secrets_count * 4)  # –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ 1:4
                fixed_non_secrets = fixed_non_secrets[:target_non_secrets]
                fixes_applied.append(f"–£—Ä–µ–∑–∞–Ω—ã –Ω–µ-—Å–µ–∫—Ä–µ—Ç—ã –¥–ª—è –±–∞–ª–∞–Ω—Å–∞ (–æ—Å—Ç–∞–ª–æ—Å—å {target_non_secrets})")
            elif balance > 0.8:  # –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —Å–µ–∫—Ä–µ—Ç–æ–≤
                # –£—Ä–µ–∑–∞–µ–º —Å–µ–∫—Ä–µ—Ç—ã
                target_secrets = min(secrets_count, non_secrets_count * 4)  # –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ 4:1
                fixed_secrets = fixed_secrets[:target_secrets]
                fixes_applied.append(f"–£—Ä–µ–∑–∞–Ω—ã —Å–µ–∫—Ä–µ—Ç—ã –¥–ª—è –±–∞–ª–∞–Ω—Å–∞ (–æ—Å—Ç–∞–ª–æ—Å—å {target_secrets})")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã
        print("–°–æ—Ö—Ä–∞–Ω—è—é –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã...")
        
        with open(self.secrets_path, 'w', encoding='utf-8') as f:
            for secret in tqdm(fixed_secrets, desc="–°–æ—Ö—Ä–∞–Ω—è—é —Å–µ–∫—Ä–µ—Ç—ã"):
                f.write(secret + '\n')
        
        with open(self.non_secrets_path, 'w', encoding='utf-8') as f:
            for non_secret in tqdm(fixed_non_secrets, desc="–°–æ—Ö—Ä–∞–Ω—è—é –Ω–µ-—Å–µ–∫—Ä–µ—Ç—ã"):
                f.write(non_secret + '\n')
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–µ
        self.secrets = fixed_secrets
        self.non_secrets = fixed_non_secrets
        
        # –í—ã–≤–æ–¥–∏–º –æ—Ç—á–µ—Ç –æ–± –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è—Ö
        print("\n" + "="*60)
        print("–û–¢–ß–ï–¢ –û–ë –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø–•")
        print("="*60)
        
        original_total = self.analysis_results['basic_statistics']['total_count']
        new_total = len(fixed_secrets) + len(fixed_non_secrets)
        
        print(f"–ò—Å—Ö–æ–¥–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–æ–≤: {original_total}")
        print(f"–ù–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–æ–≤: {new_total}")
        print(f"–£–¥–∞–ª–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {original_total - new_total}")
        print(f"–ù–æ–≤—ã–π –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤: {len(fixed_secrets)}/{len(fixed_non_secrets)} " +
              f"({len(fixed_secrets)/(len(fixed_secrets) + len(fixed_non_secrets)):.1%} —Å–µ–∫—Ä–µ—Ç–æ–≤)")
        print(f"–†–µ–∑–µ—Ä–≤–Ω—ã–µ –∫–æ–ø–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {backup_dir}")
        
        print("\n–ü—Ä–∏–º–µ–Ω—ë–Ω–Ω—ã–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:")
        for fix in fixes_applied:
            print(f"  ‚úì {fix}")
        
        print(f"\n–î–∞—Ç–∞—Å–µ—Ç—ã –æ–±–Ω–æ–≤–ª–µ–Ω—ã:")
        print(f"  - {self.secrets_path}")
        print(f"  - {self.non_secrets_path}")
    
    def generate_report(self) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç –æ–± –∞–Ω–∞–ª–∏–∑–µ –∫–∞—á–µ—Å—Ç–≤–∞"""
        # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∞–Ω–∞–ª–∏–∑—ã
        self.analyze_basic_statistics()
        self.analyze_duplicates()
        self.analyze_rule_coverage()
        self.analyze_data_quality_issues()
        self.analyze_pattern_distribution()
        recommendations = self.generate_recommendations()
        
        report = []
        report.append("=" * 80)
        report.append("–û–¢–ß–ï–¢ –û –ö–ê–ß–ï–°–¢–í–ï –î–ê–¢–ê–°–ï–¢–û–í –î–õ–Ø –î–ï–¢–ï–ö–¶–ò–ò –°–ï–ö–†–ï–¢–û–í")
        report.append("=" * 80)
        report.append("")
        
        report.append(f"–û–ë–©–ê–Ø –û–¶–ï–ù–ö–ê –ö–ê–ß–ï–°–¢–í–ê: {recommendations['quality_score_10']:.1f}/10")
        
        # –î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ü–µ–Ω–∫–∏
        detailed_scores = recommendations.get('detailed_scores', {})
        score_labels = {
            'dataset_size': '–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞',
            'class_balance': '–ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤', 
            'duplicates': '–û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤',
            'cross_class': '–û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π',
            'rule_coverage': '–ü–æ–∫—Ä—ã—Ç–∏–µ –ø—Ä–∞–≤–∏–ª–∞–º–∏',
            'data_quality': '–ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö'
        }
        
        report.append("\n–î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ü–µ–Ω–∫–∏:")
        for criterion, score in detailed_scores.items():
            label = score_labels.get(criterion, criterion)
            report.append(f"  {label}: {score:.1f}/10")
        
        # –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        score_10 = recommendations['quality_score_10']
        if score_10 >= 8.5:
            quality_label = "–û–¢–õ–ò–ß–ù–û"
        elif score_10 >= 7.0:
            quality_label = "–•–û–†–û–®–û"
        elif score_10 >= 5.5:
            quality_label = "–£–î–û–í–õ–ï–¢–í–û–†–ò–¢–ï–õ–¨–ù–û"
        elif score_10 >= 3.0:
            quality_label = "–ü–õ–û–•–û"
        else:
            quality_label = "–ö–†–ò–¢–ò–ß–ù–û"
        
        report.append(f"\n–ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: {quality_label}")
        report.append("")
        
        # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats = self.analysis_results['basic_statistics']
        report.append("1. –ë–ê–ó–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        report.append("-" * 20)
        report.append(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤: {stats['total_count']:,}")
        report.append(f"–°–µ–∫—Ä–µ—Ç—ã: {stats['secrets_count']:,}")
        report.append(f"–ù–µ-—Å–µ–∫—Ä–µ—Ç—ã: {stats['non_secrets_count']:,}")
        report.append(f"–ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤: {stats['class_balance']:.1%} —Å–µ–∫—Ä–µ—Ç–æ–≤")
        report.append(f"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å–µ–∫—Ä–µ—Ç–æ–≤: {stats['secrets_avg_length']:.1f} ¬± {stats['secrets_length_std']:.1f}")
        report.append(f"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –Ω–µ-—Å–µ–∫—Ä–µ—Ç–æ–≤: {stats['non_secrets_avg_length']:.1f} ¬± {stats['non_secrets_length_std']:.1f}")
        report.append("")
        
        # –î—É–±–ª–∏–∫–∞—Ç—ã
        duplicates = self.analysis_results['duplicates']
        report.append("2. –ê–ù–ê–õ–ò–ó –î–£–ë–õ–ò–ö–ê–¢–û–í")
        report.append("-" * 18)
        report.append(f"–¢–æ—á–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã –≤ —Å–µ–∫—Ä–µ—Ç–∞—Ö: {duplicates['secrets_exact_duplicates']:,}")
        report.append(f"–¢–æ—á–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã –≤ –Ω–µ-—Å–µ–∫—Ä–µ—Ç–∞—Ö: {duplicates['non_secrets_exact_duplicates']:,}")
        report.append(f"–ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏: {duplicates['cross_class_duplicates']:,} ‚ö†Ô∏è")
        
        if duplicates['duplicate_examples']['cross_class']:
            report.append("\n–ü—Ä–∏–º–µ—Ä—ã –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏:")
            for example in duplicates['duplicate_examples']['cross_class']:
                report.append(f"  ‚Ä¢ {example}")
        report.append("")
        
        # –ü–æ–∫—Ä—ã—Ç–∏–µ –ø—Ä–∞–≤–∏–ª–∞–º–∏
        coverage = self.analysis_results['rule_coverage']
        report.append("3. –ü–û–ö–†–´–¢–ò–ï –ü–†–ê–í–ò–õ–ê–ú–ò")
        report.append("-" * 19)
        report.append(f"–í—Å–µ–≥–æ –ø—Ä–∞–≤–∏–ª: {coverage['total_rules']}")
        report.append(f"–ü–æ–∫—Ä—ã—Ç–∏–µ —Å–µ–∫—Ä–µ—Ç–æ–≤: {coverage['secrets_coverage_ratio']:.1%}")
        report.append(f"–ü–æ–∫—Ä—ã—Ç–∏–µ –Ω–µ-—Å–µ–∫—Ä–µ—Ç–æ–≤: {coverage['non_secrets_coverage_ratio']:.1%}")
        report.append(f"–ù–µ–æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ —Å–µ–∫—Ä–µ—Ç—ã: {coverage['uncovered_secrets_count']:,}")
        report.append(f"–ù–µ–æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –Ω–µ-—Å–µ–∫—Ä–µ—Ç—ã: {coverage['uncovered_non_secrets_count']:,}")
        
        if coverage['uncovered_examples']['secrets']:
            report.append("\n–ü—Ä–∏–º–µ—Ä—ã –Ω–µ–æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö —Å–µ–∫—Ä–µ—Ç–æ–≤:")
            for example in coverage['uncovered_examples']['secrets']:
                report.append(f"  ‚Ä¢ {example}")
        report.append("")
        
        # –ü—Ä–æ–±–ª–µ–º—ã –∫–∞—á–µ—Å—Ç–≤–∞
        quality = self.analysis_results['quality_issues']
        report.append("4. –ü–†–û–ë–õ–ï–ú–´ –ö–ê–ß–ï–°–¢–í–ê –î–ê–ù–ù–´–•")
        report.append("-" * 27)
        report.append(f"–í—Å–µ–≥–æ –ø—Ä–æ–±–ª–µ–º: {quality['total_issues']:,}")
        for issue_type, count in quality['issues_summary'].items():
            if count > 0:
                issue_name = {
                    'empty_strings': '–ü—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏',
                    'too_short': '–°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ',
                    'too_long': '–°–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ',
                    'non_printable_chars': '–ù–µ–ø–µ—á–∞—Ç–∞–µ–º—ã–µ —Å–∏–º–≤–æ–ª—ã',
                    'encoding_issues': '–ü—Ä–æ–±–ª–µ–º—ã –∫–æ–¥–∏—Ä–æ–≤–∫–∏',
                    'whitespace_only': '–¢–æ–ª—å–∫–æ –ø—Ä–æ–±–µ–ª—ã',
                    'potential_test_data': '–í–æ–∑–º–æ–∂–Ω—ã–µ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ'
                }.get(issue_type, issue_type)
                report.append(f"  {issue_name}: {count:,}")
        report.append("")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        report.append("5. –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò")
        report.append("-" * 14)
        
        if recommendations['critical_issues']:
            report.append("üî¥ –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ü–†–û–ë–õ–ï–ú–´:")
            for issue in recommendations['critical_issues']:
                report.append(f"  ‚Ä¢ {issue}")
            report.append("")
        
        if recommendations['warnings']:
            report.append("üü° –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–Ø:")
            for warning in recommendations['warnings']:
                report.append(f"  ‚Ä¢ {warning}")
            report.append("")
        
        if recommendations['suggestions']:
            report.append("üí° –ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø –ü–û –£–õ–£–ß–®–ï–ù–ò–Æ:")
            for suggestion in recommendations['suggestions']:
                report.append(f"  ‚Ä¢ {suggestion}")
            report.append("")
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø—Ä–∞–≤–∏–º—ã–µ –ø—Ä–æ–±–ª–µ–º—ã
        fixable = recommendations.get('fixable_issues', {})
        if fixable:
            report.append("üîß –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò –ò–°–ü–†–ê–í–ò–ú–´–ï –ü–†–û–ë–õ–ï–ú–´:")
            if fixable.get('cross_class_duplicates'):
                report.append("  ‚Ä¢ –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏ –º–æ–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å")
            if fixable.get('duplicates'):
                report.append("  ‚Ä¢ –î—É–±–ª–∏–∫–∞—Ç—ã –º–æ–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å")
            if fixable.get('quality_issues'):
                report.append("  ‚Ä¢ –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ –º–æ–∂–Ω–æ –æ—á–∏—Å—Ç–∏—Ç—å")
            report.append("")
        
        # –ó–∞–∫–ª—é—á–µ–Ω–∏–µ
        report.append("=" * 80)
        report.append("–ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï")
        report.append("=" * 80)
        
        score_10 = recommendations['quality_score_10']
        if score_10 >= 8.5:
            report.append("–î–∞—Ç–∞—Å–µ—Ç—ã –∏–º–µ—é—Ç –æ—Ç–ª–∏—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∏ –ø–æ–ª–Ω–æ—Å—Ç—å—é –≥–æ—Ç–æ–≤—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.")
        elif score_10 >= 7.0:
            report.append("–î–∞—Ç–∞—Å–µ—Ç—ã –∏–º–µ—é—Ç —Ö–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ —Å –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–º–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∞–º–∏.")
        elif score_10 >= 5.5:
            report.append("–î–∞—Ç–∞—Å–µ—Ç—ã –∏–º–µ—é—Ç —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, —Ä–µ–∫–æ–º–µ–Ω–¥—É—é—Ç—Å—è —É–ª—É—á—à–µ–Ω–∏—è.")
        elif score_10 >= 3.0:
            report.append("–î–∞—Ç–∞—Å–µ—Ç—ã –∏–º–µ—é—Ç –ø–ª–æ—Ö–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∏ —Ç—Ä–µ–±—É—é—Ç —Å–µ—Ä—å–µ–∑–Ω—ã—Ö –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π.")
        else:
            report.append("–î–∞—Ç–∞—Å–µ—Ç—ã –∏–º–µ—é—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –Ω–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∏ —Ç—Ä–µ–±—É—é—Ç –ø–æ–ª–Ω–æ–π –ø–µ—Ä–µ—Ä–∞–±–æ—Ç–∫–∏.")
        
        report.append("")
        report.append("–û—Ü–µ–Ω–∫–∞ –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ 6 –∫–ª—é—á–µ–≤—ã—Ö –∫—Ä–∏—Ç–µ—Ä–∏—è—Ö:")
        report.append("‚Ä¢ –†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞ (15% –≤–µ—Å–∞)")
        report.append("‚Ä¢ –ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ (15% –≤–µ—Å–∞)")  
        report.append("‚Ä¢ –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (15% –≤–µ—Å–∞)")
        report.append("‚Ä¢ –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏ (25% –≤–µ—Å–∞) - –∫—Ä–∏—Ç–∏—á–Ω–æ!")
        report.append("‚Ä¢ –ü–æ–∫—Ä—ã—Ç–∏–µ –ø—Ä–∞–≤–∏–ª–∞–º–∏ (20% –≤–µ—Å–∞)")
        report.append("‚Ä¢ –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö (10% –≤–µ—Å–∞)")
        report.append("")
        report.append("–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –æ—Å–Ω–æ–≤–∞–Ω—ã –Ω–∞ –ª—É—á—à–∏—Ö –ø—Ä–∞–∫—Ç–∏–∫–∞—Ö –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è")
        report.append("–∏ —Å–ø–µ—Ü–∏—Ñ–∏–∫–µ –∑–∞–¥–∞—á –¥–µ—Ç–µ–∫—Ü–∏–∏ —Å–µ–∫—Ä–µ—Ç–æ–≤ –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –∫–æ–¥–µ.")
        
        return "\n".join(report)
    
    def save_analysis_json(self, filepath: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –≤ JSON —Ñ–∞–π–ª"""
        # –£–±–∏—Ä–∞–µ–º –±–æ–ª—å—à–∏–µ —Å–ø–∏—Å–∫–∏ –∏–∑ JSON –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –º–µ—Å—Ç–∞
        analysis_copy = self.analysis_results.copy()
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä —Å–æ—Ö—Ä–∞–Ω—è–µ–º—ã—Ö —Å–ø–∏—Å–∫–æ–≤
        if 'duplicates' in analysis_copy:
            analysis_copy['duplicates'].pop('secrets_duplicates_dict', None)
            analysis_copy['duplicates'].pop('non_secrets_duplicates_dict', None)
            analysis_copy['duplicates'].pop('cross_class_list', None)
        
        if 'rule_coverage' in analysis_copy:
            analysis_copy['rule_coverage'].pop('uncovered_secrets_list', None)
            analysis_copy['rule_coverage'].pop('uncovered_non_secrets_list', None)
        
        if 'quality_issues' in analysis_copy:
            analysis_copy['quality_issues'].pop('issues_full', None)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(analysis_copy, f, ensure_ascii=False, indent=2)


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∞–Ω–∞–ª–∏–∑–∞"""
    # –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º (–Ω–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–æ–¥ –≤–∞—à–∏ –ø—É—Ç–∏)
    rules_path = "../Settings/rules.yml"
    secrets_path = "../Datasets/Dataset_Secrets.txt"
    non_secrets_path = "../Datasets/Dataset_NonSecrets.txt"
    
    print("–ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ —Å–µ–∫—Ä–µ—Ç–æ–≤...")
    print("=" * 60)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤
    missing_files = []
    for path, name in [(rules_path, "rules.yml"), 
                       (secrets_path, "Dataset_Secrets.txt"), 
                       (non_secrets_path, "Dataset_NonSecrets.txt")]:
        if not os.path.exists(path):
            missing_files.append((path, name))
    
    if missing_files:
        print("–û–®–ò–ë–ö–ê: –ù–µ –Ω–∞–π–¥–µ–Ω—ã —Å–ª–µ–¥—É—é—â–∏–µ —Ñ–∞–π–ª—ã:")
        for path, name in missing_files:
            print(f"  - {name}: {path}")
        print("\n–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É—Ç–∏ –∏ —Å–æ–∑–¥–∞–π—Ç–µ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ —Ñ–∞–π–ª—ã.")
        return
    
    try:
        # –°–æ–∑–¥–∞–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
        analyzer = DatasetQualityAnalyzer(rules_path, secrets_path, non_secrets_path)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
        print("\n–ó–∞–ø—É—Å–∫–∞—é –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑...")
        report = analyzer.generate_report()
        
        # –í—ã–≤–æ–¥–∏–º –æ—Ç—á–µ—Ç
        print("\n" + report)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        print("\n–°–æ—Ö—Ä–∞–Ω—è—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞...")
        analyzer.save_analysis_json("dataset_analysis_results.json")
        print("–î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: dataset_analysis_results.json")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç –≤ —Ñ–∞–π–ª
        with open("dataset_quality_report.txt", "w", encoding="utf-8") as f:
            f.write(report)
        print("–û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: dataset_quality_report.txt")
        
        # –ü—Ä–µ–¥–ª–∞–≥–∞–µ–º –∏—Å–ø—Ä–∞–≤–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç—ã
        recommendations = analyzer.analysis_results.get('recommendations', {})
        fixable_issues = recommendations.get('fixable_issues', {})
        
        if fixable_issues:
            print("\n" + "="*60)
            print("–ù–ê–ô–î–ï–ù–´ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò –ò–°–ü–†–ê–í–ò–ú–´–ï –ü–†–û–ë–õ–ï–ú–´!")
            print("="*60)
            
            if fixable_issues.get('cross_class_duplicates'):
                print("üî¥ –ö—Ä–∏—Ç–∏—á–Ω–æ: –Ω–∞–π–¥–µ–Ω—ã –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏")
            if fixable_issues.get('duplicates'):
                print("‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω—ã –¥—É–±–ª–∏–∫–∞—Ç—ã")
            if fixable_issues.get('quality_issues'):
                print("‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω—ã –ø—Ä–æ–±–ª–µ–º—ã –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö")
            
            print("\n–≠—Ç–∏ –ø—Ä–æ–±–ª–µ–º—ã –º–æ–≥—É—Ç —Å–µ—Ä—å–µ–∑–Ω–æ –ø–æ–≤–ª–∏—è—Ç—å –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.")
            
            while True:
                choice = input("\n–•–æ—Ç–∏—Ç–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø—Ä–∞–≤–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç—ã? (y/n): ").lower().strip()
                if choice in ['y', 'yes', '–¥–∞', '–¥']:
                    analyzer.fix_datasets()
                    
                    # –ü—Ä–µ–¥–ª–∞–≥–∞–µ–º –ø–æ–≤—Ç–æ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑
                    reanalyze = input("\n–•–æ—Ç–∏—Ç–µ –ø—Ä–æ–≤–µ—Å—Ç–∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤? (y/n): ").lower().strip()
                    if reanalyze in ['y', 'yes', '–¥–∞', '–¥']:
                        print("\n" + "="*60)
                        print("–ü–û–í–¢–û–†–ù–´–ô –ê–ù–ê–õ–ò–ó –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–• –î–ê–¢–ê–°–ï–¢–û–í")
                        print("="*60)
                        
                        # –û—á–∏—â–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                        analyzer.analysis_results = {}
                        
                        # –ü—Ä–æ–≤–æ–¥–∏–º –Ω–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑
                        new_report = analyzer.generate_report()
                        print("\n" + new_report)
                        
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                        analyzer.save_analysis_json("dataset_analysis_results_fixed.json")
                        with open("dataset_quality_report_fixed.txt", "w", encoding="utf-8") as f:
                            f.write(new_report)
                        
                        print("\n–û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
                        print("- dataset_analysis_results_fixed.json")
                        print("- dataset_quality_report_fixed.txt")
                    
                    break
                elif choice in ['n', 'no', '–Ω–µ—Ç', '–Ω']:
                    print("–î–∞—Ç–∞—Å–µ—Ç—ã –Ω–µ –∏–∑–º–µ–Ω–µ–Ω—ã. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø—Ä–∞–≤–∏—Ç—å –ø—Ä–æ–±–ª–µ–º—ã –≤—Ä—É—á–Ω—É—é.")
                    break
                else:
                    print("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ 'y' (–¥–∞) –∏–ª–∏ 'n' (–Ω–µ—Ç)")
        else:
            print("\n‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø—Ä–∞–≤–∏–º—ã—Ö –ø—Ä–æ–±–ª–µ–º –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
        
        print(f"\nüéâ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–∞–π–ª—ã —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏.")
        
    except KeyboardInterrupt:
        print("\n\n–ê–Ω–∞–ª–∏–∑ –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.")
    except Exception as e:
        print(f"\n–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
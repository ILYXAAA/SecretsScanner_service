import asyncio
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from app.models import ScanRequest
from app.repo_utils import download_repo, delete_dir
import aiohttp
import os
import tempfile
import multiprocessing
from typing import Tuple
from dotenv import load_dotenv
import zipfile
import time
import gzip
import base64
import logging
import json
import traceback
from logging.handlers import RotatingFileHandler

# Setup logging to file
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        RotatingFileHandler('secrets_scanner_service.log', maxBytes=10*1024*1024, backupCount=5, encoding='utf-8'),
        logging.StreamHandler()  # –¢–∞–∫–∂–µ –≤—ã–≤–æ–¥–∏—Ç—å –≤ –∫–æ–Ω—Å–æ–ª—å
    ]
)
logger = logging.getLogger("queue_worker")

# Load environment variables
load_dotenv()
task_queue = asyncio.Queue()

HubType = os.getenv("HubType")

# Thread pool for I/O operations (downloads)
download_executor = ThreadPoolExecutor(max_workers=5)

# Process pool for CPU-intensive operations (model inference)
model_executor = ProcessPoolExecutor(max_workers=multiprocessing.cpu_count())

async def add_to_queue_background(request: ScanRequest, commit: str):
    await task_queue.put((request, commit))
    logger.info(f"–ü—Ä–æ–µ–∫—Ç {request.ProjectName} –ø–æ—Å—Ç–∞–≤–ª–µ–Ω –≤ –æ—á–µ—Ä–µ–¥—å –Ω–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ")

async def add_multi_scan_to_queue(multi_scan_items: list, commits: list):
    """Add multi-scan sequence to queue"""
    await task_queue.put(("multi_scan", multi_scan_items, commits))
    logger.info(f"–ú—É–ª—å—Ç–∏—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ {len(multi_scan_items)} –ø—Ä–æ–µ–∫—Ç–æ–≤ –ø–æ—Å—Ç–∞–≤–ª–µ–Ω–æ –≤ –æ—á–µ—Ä–µ–¥—å")

async def start_worker():
    """Worker that processes requests concurrently"""
    while True:
        try:
            # –î–æ–±–∞–≤–ª—è–µ–º timeout –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –≤–µ—á–Ω–æ–≥–æ –æ–∂–∏–¥–∞–Ω–∏—è
            item = await asyncio.wait_for(task_queue.get(), timeout=5.0)
            
            # Check item type
            if isinstance(item, tuple) and len(item) == 3:
                if item[0] == "multi_scan":
                    # Multi-scan processing
                    _, multi_scan_items, commits = item
                    asyncio.create_task(process_multi_scan_sequence(multi_scan_items, commits))
                elif item[0] == "local_scan":
                    # Local scan processing
                    _, request_dict, zip_content = item
                    asyncio.create_task(process_local_scan_async(request_dict, zip_content))
            else:
                # Single scan processing
                request, commit = item
                asyncio.create_task(process_request_async(request, commit))
            
            task_queue.task_done()
        except asyncio.TimeoutError:
            # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –Ω—É–∂–Ω–æ –ª–∏ –∑–∞–≤–µ—Ä—à–∏—Ç—å—Å—è
            continue
        except asyncio.CancelledError:
            logger.info("Worker –ø–æ–ª—É—á–∏–ª —Å–∏–≥–Ω–∞–ª –æ—Ç–º–µ–Ω—ã")
            break
        except Exception as e:
            logger.error(f"Worker error: {e}")
            await asyncio.sleep(1)

async def process_local_scan_async(request_dict: dict, zip_content: bytes):
    """Process uploaded zip file locally"""
    start_time = time.time()
    temp_dir = tempfile.mkdtemp(dir=os.getenv("TEMP_DIR", "C:\\"))
    
    try:
        project_name = request_dict["ProjectName"]
        callback_url = request_dict["CallbackUrl"]
        commit = request_dict["Ref"]
        
        logger.info(f"–ù–∞—á–∏–Ω–∞—é –ª–æ–∫–∞–ª—å–Ω–æ–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ {project_name}")
        
        # Save zip content to file
        zip_save_start = time.time()
        zip_path = os.path.join(temp_dir, f"{project_name}.zip")
        with open(zip_path, 'wb') as f:
            f.write(zip_content)
        
        logger.info(f"ZIP —Ñ–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {project_name} (–≤—Ä–µ–º—è: {time.time() - zip_save_start:.2f}—Å)")
        
        # Extract zip file
        extract_start = time.time()
        extracted_path = os.path.join(temp_dir, "extracted")
        os.makedirs(extracted_path, exist_ok=True)
        
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(
            download_executor,
            extract_zip_file,
            zip_path,
            extracted_path
        )
        
        logger.info(f"ZIP —Ñ–∞–π–ª —Ä–∞—Å–ø–∞–∫–æ–≤–∞–Ω: {project_name} (–≤—Ä–µ–º—è: {time.time() - extract_start:.2f}—Å)")
        
        # Scan extracted content
        scan_start = time.time()
        logger.info(f"–°–∫–∞–Ω–∏—Ä—É—é {project_name}")
        
        results, all_files_count = await loop.run_in_executor(
            model_executor,
            scan_repo_with_model,
            extracted_path,
            project_name,
            request_dict
        )
        
        scan_time = time.time() - scan_start
        logger.info(f"–ü—Ä–æ—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–æ {project_name} (–≤—Ä–µ–º—è: {scan_time:.2f}—Å, —Ñ–∞–π–ª–æ–≤: {all_files_count})")
        
        # Send results
        payload = {
            "Status": "completed",
            "Message": "Scanned Successfully",
            "ProjectName": project_name,
            "ProjectRepoUrl": request_dict["RepoUrl"],
            "RepoCommit": commit,
            "Results": results,
            "FilesScanned": all_files_count
        }
        
        await send_callback(callback_url, payload)
        
        total_time = time.time() - start_time
        logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã {project_name} –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω—ã –Ω–∞ CallBack (–æ–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.2f}—Å)")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ª–æ–∫–∞–ª—å–Ω–æ–º —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏ {request_dict.get('ProjectName', 'unknown')}: {e}")
        await send_error_callback(request_dict.get("CallbackUrl", ""), str(e))
    finally:
        # Cleanup
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(download_executor, delete_dir, temp_dir)

def extract_zip_file(zip_path: str, extract_path: str):
    """Extract zip file synchronously"""
    try:
        with zipfile.ZipFile(zip_path, 'r') as zip_file:
            zip_file.extractall(extract_path)
        return True
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å–ø–∞–∫–æ–≤–∫–µ ZIP: {e}")
        raise e

async def process_multi_scan_sequence(multi_scan_items: list, commits: list):
    """Process multi-scan repositories sequentially"""
    multi_start_time = time.time()
    logger.info(f"–ù–∞—á–∏–Ω–∞—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –º—É–ª—å—Ç–∏—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ {len(multi_scan_items)} —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤")
    
    for i, (item_dict, commit) in enumerate(zip(multi_scan_items, commits)):
        try:
            # Convert dict back to ScanRequest
            from app.models import ScanRequest
            request = ScanRequest(**item_dict)
            
            item_start = time.time()
            logger.info(f"–ú—É–ª—å—Ç–∏—Å–∫–∞–Ω [{i+1}/{len(multi_scan_items)}]: {request.ProjectName}")
            
            # Process sequentially (wait for completion)
            await process_request_sequential(request, commit)
            
            item_time = time.time() - item_start
            logger.info(f"–ú—É–ª—å—Ç–∏—Å–∫–∞–Ω [{i+1}/{len(multi_scan_items)}] –∑–∞–≤–µ—Ä—à–µ–Ω: {request.ProjectName} (–≤—Ä–µ–º—è: {item_time:.2f}—Å)")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ –º—É–ª—å—Ç–∏—Å–∫–∞–Ω–µ [{i+1}/{len(multi_scan_items)}]: {e}")
            # Continue with next repository even if one fails
            try:
                if 'request' in locals():
                    await send_error_callback(request.CallbackUrl, f"–û—à–∏–±–∫–∞ –º—É–ª—å—Ç–∏—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è: {str(e)}")
            except:
                pass
    
    total_multi_time = time.time() - multi_start_time
    logger.info(f"–ú—É–ª—å—Ç–∏—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {len(multi_scan_items)} —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤ (–æ–±—â–µ–µ –≤—Ä–µ–º—è: {total_multi_time:.2f}—Å)")

async def process_request_sequential(request: ScanRequest, commit: str):
    """Sequential processing for multi-scan (blocks until complete)"""
    temp_dir = tempfile.mkdtemp(dir=os.getenv("TEMP_DIR", "C:\\"))
    
    try:
        # Step 1: Download repository
        download_start = time.time()
        logger.info(f"–°–∫–∞—á–∏–≤–∞—é {request.ProjectName}")
        loop = asyncio.get_event_loop()
        
        extracted_repo_path, status_message = await loop.run_in_executor(
            download_executor, 
            download_repo_sync, 
            request.RepoUrl, 
            commit, 
            temp_dir
        )
        
        if not extracted_repo_path:
            await send_error_callback(request.CallbackUrl, status_message)
            return
            
        download_time = time.time() - download_start
        logger.info(f"–°–∫–∞—á–∞–Ω–æ {request.ProjectName} (–≤—Ä–µ–º—è: {download_time:.2f}—Å)")
        
        # Step 2: Scan repository
        scan_start = time.time()
        logger.info(f"–°–∫–∞–Ω–∏—Ä—É—é {request.ProjectName}")
        
        request_dict = {
            "ProjectName": request.ProjectName,
            "RepoUrl": request.RepoUrl,
            "RefType": request.RefType,
            "Ref": request.Ref,
            "CallbackUrl": request.CallbackUrl
        }
        
        results, all_files_count = await loop.run_in_executor(
            model_executor,
            scan_repo_with_model,
            extracted_repo_path,
            request.ProjectName,
            request_dict
        )
        
        scan_time = time.time() - scan_start
        logger.info(f"–ü—Ä–æ—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–æ {request.ProjectName} (–≤—Ä–µ–º—è: {scan_time:.2f}—Å, —Ñ–∞–π–ª–æ–≤: {all_files_count})")
        
        # Step 3: Send results
        payload = {
            "Status": "completed",
            "Message": "Scanned Successfully",
            "ProjectName": request.ProjectName,
            "ProjectRepoUrl": request.RepoUrl,
            "RepoCommit": commit,
            "Results": results,
            "FilesScanned": all_files_count
        }
        
        await send_callback(request.CallbackUrl, payload)
        logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω—ã –¥–ª—è {request.ProjectName}")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {request.ProjectName}: {e}")
        await send_error_callback(request.CallbackUrl, str(e))
    finally:
        # Cleanup
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(download_executor, delete_dir, temp_dir)

def download_repo_sync(repo_url: str, commit: str, temp_dir: str) -> Tuple[str, str]:
    """Synchronous wrapper for download_repo to run in thread pool"""
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        result = loop.run_until_complete(download_repo(repo_url, commit, temp_dir))
        return result
    finally:
        loop.close()

def scan_repo_with_model(repo_path: str, project_name: str, request_dict: dict) -> Tuple[list, int]:
    """Process scanning and model inference in separate process"""
    import sys
    import os
    import asyncio
    
    # Add the project root to Python path
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(current_dir)
    if project_root not in sys.path:
        sys.path.insert(0, project_root)
    
    try:
        from app.scanner import scan_repo_without_callback
        from app.model_loader import get_model_instance, filter_secrets_in_process
        from app.models import ScanRequest
        
        # Recreate request object from dict
        request = ScanRequest(**request_dict)
        
        # Perform scanning without model (in process)
        results, file_count = asyncio.run(scan_repo_without_callback(request, repo_path, project_name))
        
        # Apply model filtering
        filtered_results = filter_secrets_in_process(results)
        
        return filtered_results, file_count
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
        # Return empty results with error info
        return [{"error": str(e), "path": "process_error", "severity": "High", "Type": "Process Error"}], 0

async def process_request_async(request: ScanRequest, commit: str):
    """Async processing with concurrent download and scanning"""
    start_time = time.time()
    temp_dir = tempfile.mkdtemp(dir=os.getenv("TEMP_DIR", "C:\\"))
    
    try:
        # Step 1: Download repository in thread pool (non-blocking)
        download_start = time.time()
        logger.info(f"–ù–∞—á–∏–Ω–∞—é —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ {request.ProjectName}")
        loop = asyncio.get_event_loop()
        
        extracted_repo_path, status_message = await loop.run_in_executor(
            download_executor, 
            download_repo_sync, 
            request.RepoUrl, 
            commit, 
            temp_dir
        )
        
        if not extracted_repo_path:
            await send_error_callback(request.CallbackUrl, status_message)
            return
            
        download_time = time.time() - download_start
        logger.info(f"–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ {request.ProjectName} (–≤—Ä–µ–º—è: {download_time:.2f}—Å)")
        
        # Step 2: Scan repository with model in process pool (CPU-intensive)
        scan_start = time.time()
        logger.info(f"–ù–∞—á–∏–Ω–∞—é —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ {request.ProjectName}")
        
        # Convert request to dict for multiprocessing
        request_dict = {
            "ProjectName": request.ProjectName,
            "RepoUrl": request.RepoUrl,
            "RefType": request.RefType,
            "Ref": request.Ref,
            "CallbackUrl": request.CallbackUrl
        }
        
        results, all_files_count = await loop.run_in_executor(
            model_executor,
            scan_repo_with_model,
            extracted_repo_path,
            request.ProjectName,
            request_dict
        )
        
        scan_time = time.time() - scan_start
        logger.info(f"–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ {request.ProjectName} (–≤—Ä–µ–º—è: {scan_time:.2f}—Å, —Ñ–∞–π–ª–æ–≤: {all_files_count})")
        
        # Step 3: Send results
        payload = {
            "Status": "completed",
            "Message": "Scanned Successfully",
            "ProjectName": request.ProjectName,
            "ProjectRepoUrl": request.RepoUrl,
            "RepoCommit": commit,
            "Results": results,
            "FilesScanned": all_files_count
        }
        
        await send_callback(request.CallbackUrl, payload)
        
        total_time = time.time() - start_time
        logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω—ã –¥–ª—è {request.ProjectName} (–æ–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.2f}—Å)")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {request.ProjectName}: {e}")
        await send_error_callback(request.CallbackUrl, str(e))
    finally:
        # Cleanup in thread pool to avoid blocking
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(download_executor, delete_dir, temp_dir)

async def send_callback(callback_url: str, payload: dict):
    """Send callback with compression support"""
    
    project_name = payload.get("ProjectName", "unknown")
    results_count = len(payload.get("Results", []))
    
    # –°–µ—Ä–∏–∞–ª–∏–∑—É–µ–º payload
    payload_json = json.dumps(payload, ensure_ascii=False)
    original_size = len(payload_json.encode('utf-8'))
    
    # –°–∂–∏–º–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    compressed_data = gzip.compress(payload_json.encode('utf-8'))
    compressed_size = len(compressed_data)
    
    # –ö–æ–¥–∏—Ä—É–µ–º –≤ base64 –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏
    compressed_b64 = base64.b64encode(compressed_data).decode('ascii')
    
    # –°–æ–∑–¥–∞–µ–º —Å–∂–∞—Ç—ã–π payload
    compressed_payload = {
        "compressed": True,
        "data": compressed_b64,
        "original_size": original_size,
        "compressed_size": compressed_size
    }
    
    compressed_json = json.dumps(compressed_payload)
    final_size = len(compressed_json.encode('utf-8'))
    
    compression_ratio = (1 - final_size / original_size) * 100
    
    logger.info(f"üì§ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º callback –¥–ª—è {project_name}")
    logger.info(f"   URL: {callback_url}")
    logger.info(f"   –û—Ä–∏–≥–∏–Ω–∞–ª: {original_size / 1024:.2f} KB")
    logger.info(f"   –°–∂–∞—Ç–æ: {final_size / 1024:.2f} KB")
    logger.info(f"   –≠–∫–æ–Ω–æ–º–∏—è: {compression_ratio:.1f}%")
    logger.info(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {results_count}")
    
    max_retries = 3
    
    for attempt in range(max_retries):
        start_time = time.time()
        logger.info(f"üîÑ –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries}")
        
        try:
            timeout = aiohttp.ClientTimeout(
                total=60,
                connect=10,
                sock_read=30
            )
            
            headers = {
                'Content-Type': 'application/json; charset=utf-8',
                'User-Agent': 'SecretsScanner-Service/1.0',
                'X-Compressed': 'gzip-base64'  # –£–∫–∞–∑—ã–≤–∞–µ–º, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ —Å–∂–∞—Ç—ã
            }
            
            async with aiohttp.ClientSession(timeout=timeout) as session:
                logger.info(f"üîó –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å {callback_url}")
                
                async with session.post(
                    callback_url,
                    data=compressed_json,
                    headers=headers
                ) as response:
                    
                    elapsed = time.time() - start_time
                    logger.info(f"üì® –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –∑–∞ {elapsed:.2f}—Å")
                    logger.info(f"   –°—Ç–∞—Ç—É—Å: {response.status} {response.reason}")
                    
                    try:
                        response_text = await response.text()
                        response_size = len(response_text)
                        logger.info(f"   –†–∞–∑–º–µ—Ä –æ—Ç–≤–µ—Ç–∞: {response_size} bytes")
                        
                        if response_size > 0:
                            preview = response_text[:200].replace('\n', '\\n')
                            logger.info(f"   –ù–∞—á–∞–ª–æ –æ—Ç–≤–µ—Ç–∞: {preview}...")
                        
                    except Exception as read_error:
                        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ç–µ–ª–∞ –æ—Ç–≤–µ—Ç–∞: {read_error}")
                        response_text = f"ERROR_READING_RESPONSE: {read_error}"
                    
                    if response.status == 200:
                        logger.info(f"‚úÖ Callback —É—Å–ø–µ—à–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –∑–∞ {elapsed:.2f}—Å (—ç–∫–æ–Ω–æ–º–∏—è {compression_ratio:.1f}%)")
                        return
                    else:
                        logger.error(f"‚ùå HTTP –æ—à–∏–±–∫–∞ {response.status}: {response.reason}")
                        
                        if response.status == 413:
                            logger.error("üí° –û—à–∏–±–∫–∞ 413: Payload —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π –¥–ª—è —Å–µ—Ä–≤–µ—Ä–∞")
                        elif response.status == 500:
                            logger.error("üí° –û—à–∏–±–∫–∞ 500: –í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ")
                        elif response.status == 502:
                            logger.error("üí° –û—à–∏–±–∫–∞ 502: –ü–ª–æ—Ö–æ–π —à–ª—é–∑ (–ø—Ä–æ–±–ª–µ–º–∞ —Å –ø—Ä–æ–∫—Å–∏)")
                        elif response.status == 503:
                            logger.error("üí° –û—à–∏–±–∫–∞ 503: –°–µ—Ä–≤–∏—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
                        elif response.status == 504:
                            logger.error("üí° –û—à–∏–±–∫–∞ 504: –¢–∞–π–º–∞—É—Ç —à–ª—é–∑–∞")
                        else:
                            logger.error(f"üí° –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π HTTP –∫–æ–¥: {response.status}")
                        
                        logger.error(f"   –ü–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç —Å–µ—Ä–≤–µ—Ä–∞: {response_text}")
        
        except asyncio.TimeoutError as e:
            elapsed = time.time() - start_time
            logger.error(f"‚è∞ –¢–∞–π–º–∞—É—Ç –ø–æ—Å–ª–µ {elapsed:.2f}—Å –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ {attempt + 1}")
            logger.error(f"   üí° –í–æ–∑–º–æ–∂–Ω–æ —Å–µ—Ä–≤–µ—Ä –Ω–µ —É—Å–ø–µ–≤–∞–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∑–∞–ø—Ä–æ—Å")
            
        except aiohttp.ClientConnectorError as e:
            elapsed = time.time() - start_time
            logger.error(f"üîå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –ø–æ—Å–ª–µ {elapsed:.2f}—Å: {e}")
            logger.error(f"   üí° –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å {callback_url}")
            
        except aiohttp.ClientOSError as e:
            elapsed = time.time() - start_time
            logger.error(f"üíª –°–∏—Å—Ç–µ–º–Ω–∞—è –æ—à–∏–±–∫–∞ –ø–æ—Å–ª–µ {elapsed:.2f}—Å: {e}")
            
        except aiohttp.ClientPayloadError as e:
            elapsed = time.time() - start_time
            logger.error(f"üì¶ –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–¥–∞—á–∏ –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ {elapsed:.2f}—Å: {e}")
            
        except aiohttp.ServerDisconnectedError as e:
            elapsed = time.time() - start_time
            logger.error(f"üîå –°–µ—Ä–≤–µ—Ä —Ä–∞–∑–æ—Ä–≤–∞–ª —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ—Å–ª–µ {elapsed:.2f}—Å: {e}")
            
        except json.JSONEncodeError as e:
            elapsed = time.time() - start_time
            logger.error(f"üìù –û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è JSON: {e}")
            break
            
        except Exception as e:
            elapsed = time.time() - start_time
            logger.error(f"‚ùì –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø–æ—Å–ª–µ {elapsed:.2f}—Å: {type(e).__name__}: {e}")
            error_traceback = traceback.format_exc()
            for line in error_traceback.split('\n'):
                if line.strip():
                    logger.error(f"      {line}")
        
        if attempt < max_retries - 1:
            wait_time = 2 ** attempt
            logger.info(f"‚è≥ –ñ–¥–µ–º {wait_time}—Å –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π...")
            await asyncio.sleep(wait_time)
    
    logger.error(f"üí• –ö–†–ò–¢–ò–ß–ù–û: –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å callback –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫")
    logger.error(f"   –ü—Ä–æ–µ–∫—Ç: {project_name}")
    logger.error(f"   URL: {callback_url}")
    logger.error(f"   –†–∞–∑–º–µ—Ä (—Å–∂–∞—Ç—ã–π): {final_size / 1024:.2f} KB")

async def send_error_callback(callback_url: str, error_message: str):
    """Send error callback"""
    payload = {
        "Status": "Error",
        "Message": error_message
    }
    await send_callback(callback_url, payload)

# Cleanup function for graceful shutdown
async def cleanup_executors():
    """Cleanup executors on shutdown"""
    try:
        logger.info("–û—á–∏—Å—Ç–∫–∞ thread pool...")
        download_executor.shutdown(wait=True, cancel_futures=True)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ download_executor: {e}")
    
    try:
        logger.info("–û—á–∏—Å—Ç–∫–∞ process pool...")
        # –î–ª—è Windows - –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤
        if hasattr(model_executor, '_processes'):
            for p in model_executor._processes.values():
                if p.is_alive():
                    p.terminate()
        model_executor.shutdown(wait=False)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ model_executor: {e}")
    
    logger.info("Cleanup –∑–∞–≤–µ—Ä—à–µ–Ω")